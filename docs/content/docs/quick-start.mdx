---
title: Quick Start
description: Generate your first embeddings with Catsu
icon: Rocket
---

Get started with Catsu in just a few steps!

## Step 1: Import and Initialize

```python
from catsu import Client

# Initialize the client (reads API keys from environment)
client = Client()
```

## Step 2: Generate Embeddings

```python
# Generate embeddings using provider:model format
response = client.embed(
    "openai:text-embedding-3-small",
    ["Hello, world!"]
)
```

## Step 3: Access Results

```python
# Access embeddings
print(response.embeddings)      # [[0.1, 0.2, 0.3, ...]]
print(response.dimensions)      # 1536

# Check usage
print(response.usage.tokens)    # 2
```

## Working with Multiple Inputs

```python
# Batch processing (more efficient)
response = client.embed(
    "openai:text-embedding-3-small",
    ["Text 1", "Text 2", "Text 3"]
)

# Results include all embeddings
print(len(response.embeddings))  # 3
print(response.usage.tokens)     # Total tokens for all inputs
```

## Single Text Input

```python
# You can also pass a single string
response = client.embed(
    "openai:text-embedding-3-small",
    "Hello, world!"
)
```

## Async Usage

For better performance with multiple requests:

```python
import asyncio
from catsu import Client

async def main():
    client = Client()

    # Single async request
    response = await client.aembed(
        "openai:text-embedding-3-small",
        "Hello, async world!"
    )

    # Parallel requests
    responses = await asyncio.gather(
        client.aembed("openai:text-embedding-3-small", "Query 1"),
        client.aembed("openai:text-embedding-3-small", "Query 2"),
        client.aembed("openai:text-embedding-3-small", "Query 3"),
    )

    print(f"Processed {len(responses)} requests in parallel")

asyncio.run(main())
```

## Using Context Managers

Context managers ensure proper cleanup:

```python
# Sync context manager
with Client() as client:
    response = client.embed("openai:text-embedding-3-small", "Hello!")
    print(response.embeddings)

# Async context manager
async with Client() as client:
    response = await client.aembed("openai:text-embedding-3-small", "Hello!")
    print(response.embeddings)
```

## Provider-Specific Parameters

Different providers support different parameters:

```python
# With input_type (for search optimization)
response = client.embed(
    "voyageai:voyage-3",
    ["What is machine learning?"],
    input_type="query"  # or "document"
)

# With custom dimensions (Matryoshka embeddings)
response = client.embed(
    "openai:text-embedding-3-small",
    ["Sample text"],
    dimensions=256
)
```

## NumPy Integration

Convert embeddings to numpy arrays:

```python
response = client.embed(
    "openai:text-embedding-3-small",
    ["Text 1", "Text 2"]
)

# Convert to numpy array
arr = response.to_numpy()
print(arr.shape)  # (2, 1536)
```

## Model Catalog

List available models:

```python
# List all models
models = client.list_models()

# Filter by provider
openai_models = client.list_models("openai")
for m in openai_models:
    print(f"{m.name}: {m.dimensions} dims")
```

## Next Steps

- [Client API](/client-api) - Learn about all client methods and configuration
- [Providers](/providers) - Explore all 11 supported providers and their parameters
- [Best Practices](/best-practices) - Optimize your usage with expert tips
