---
title: Overview
description: Overview of all 11 supported embedding providers
icon: Blocks
---

Catsu supports **11 major embedding providers** with a unified, consistent API.

## Supported Providers

| Provider | Models | Key Features | Environment Variable |
|----------|--------|--------------|---------------------|
| [OpenAI](/providers/openai) | 3 | Industry standard, Matryoshka | `OPENAI_API_KEY` |
| [Voyage AI](/providers/voyageai) | 11 | Domain-specific, multimodal, quantization | `VOYAGE_API_KEY` |
| [Cohere](/providers/cohere) | 5 | Multilingual, truncation | `COHERE_API_KEY` |
| [Gemini](/providers/gemini) | 1 | Long context, Matryoshka | `GEMINI_API_KEY` |
| [Jina AI](/providers/jinaai) | 6 | Multimodal, code-specific, long context | `JINA_API_KEY` |
| [Mistral AI](/providers/mistral) | 2 | Code-optimized, quantization | `MISTRAL_API_KEY` |
| [Nomic](/providers/nomic) | 2 | Long text handling, Matryoshka | `NOMIC_API_KEY` |
| [Cloudflare](/providers/cloudflare) | 7 | Edge inference, BGE models | `CLOUDFLARE_API_TOKEN` |
| [DeepInfra](/providers/deepinfra) | 16 | Open-source models, Qwen3 | `DEEPINFRA_API_KEY` |
| [Mixedbread](/providers/mixedbread) | 4 | Multilingual, quantization | `MIXEDBREAD_API_KEY` |
| [Together AI](/providers/togetherai) | 7 | Open-source, long context | `TOGETHER_API_KEY` |

For detailed model information including pricing and benchmarks, visit the [Models Catalog](https://catsu.dev).

## Quick Examples

### Using Different Providers

```python
from catsu import Client

client = Client()

# OpenAI
openai_response = client.embed("openai:text-embedding-3-small", ["Text"])

# Voyage AI
voyage_response = client.embed("voyageai:voyage-3", ["Text"])

# Cohere
cohere_response = client.embed("cohere:embed-v4.0", ["Text"])
```

### Provider-Specific Features

```python
# OpenAI with custom dimensions
response = client.embed(
    "openai:text-embedding-3-small",
    ["Text"],
    dimensions=256
)

# Voyage AI with input_type
response = client.embed(
    "voyageai:voyage-3",
    ["Search query"],
    input_type="query"
)

# Gemini with long context (2048 tokens)
response = client.embed(
    "gemini:gemini-embedding-001",
    ["Very long document..."]
)
```

## Choosing a Provider

Consider these factors:

- **Use case**: General retrieval, code search, multilingual, etc.
- **Features needed**: input_type, dimensions
- **Cost**: varies significantly by provider
- **Performance**: latency and throughput requirements
- **Context length**: maximum input tokens

See [Best Practices: Model Selection](/best-practices/model-selection) for detailed guidance.

## Provider Links

Explore detailed documentation for each provider:

<Cards>
  <Card title="OpenAI" href="/providers/openai">
    Industry-standard embedding models
  </Card>
  <Card title="Voyage AI" href="/providers/voyageai">
    11 models including domain-specific variants
  </Card>
  <Card title="Cohere" href="/providers/cohere">
    Multilingual embedding models
  </Card>
  <Card title="Gemini" href="/providers/gemini">
    Google's embedding model with long context
  </Card>
  <Card title="Jina AI" href="/providers/jinaai">
    Multimodal and code-specific models
  </Card>
  <Card title="Mistral AI" href="/providers/mistral">
    Code-optimized with quantization
  </Card>
  <Card title="Nomic" href="/providers/nomic">
    Matryoshka embeddings with long text support
  </Card>
  <Card title="Cloudflare" href="/providers/cloudflare">
    Edge-based inference with Workers AI
  </Card>
  <Card title="DeepInfra" href="/providers/deepinfra">
    Open-source models including Qwen3
  </Card>
  <Card title="Mixedbread" href="/providers/mixedbread">
    Multilingual with quantization support
  </Card>
  <Card title="Together AI" href="/providers/togetherai">
    Open-source models with long context
  </Card>
</Cards>
